{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torch and torchvision using pip\n",
    "# pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to open an image from a given path\n",
    "def open_image(image_path, numpy_format=False):\n",
    "    # Open the image using PIL library\n",
    "    image = Image.open(image_path)\n",
    "    # If numpy_format is True, convert the image to numpy array\n",
    "    return image if not numpy_format else np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_image('/home/glab/Desktop/kaggle/train_64/train_64/imgs/000000_64.jpg',\n",
    "           numpy_format=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING : PATH TO EDIT\n",
    "\n",
    "# Path to the images folder\n",
    "root = \"/home/glab/Desktop/kaggle/train_64/train_64/imgs/\"\n",
    "# Path to the labels file\n",
    "labels = pd.read_csv(\"/home/glab/Desktop/kaggle/train_df_challenge.csv\")\n",
    "# Path to the test images folder\n",
    "root_kaggle = \"/home/glab/Desktop/kaggle/test_64/test_64/imgs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female</th>\n",
       "      <th>Young</th>\n",
       "      <th>Attractive</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Oval_Face</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>000000_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000001_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000002_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000003_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000004_64.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Female  Young  Attractive  Smiling  Oval_Face  Wavy_Hair           path\n",
       "0     1.0    1.0         1.0      1.0        0.0        1.0  000000_64.jpg\n",
       "1     0.0    1.0         0.0      1.0        1.0        0.0  000001_64.jpg\n",
       "2     1.0    0.0         1.0      1.0        0.0        0.0  000002_64.jpg\n",
       "3     1.0    1.0         1.0      1.0        1.0        0.0  000003_64.jpg\n",
       "4     0.0    1.0         0.0      0.0        0.0        0.0  000004_64.jpg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column to the labels dataframe with the index\n",
    "labels['index'] = labels.index\n",
    "labels['path'] = labels['index'].apply(lambda index: f'{index:06}_64.jpg')\n",
    "del labels['index']\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000000_64.jpg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first path\n",
    "labels[\"path\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_image(root + labels[\"path\"].iloc[0], numpy_format=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.iloc[0, :5].to_numpy().astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.to_csv(\"../labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Young/Attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels_train, labels_test = train_test_split(labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female</th>\n",
       "      <th>Young</th>\n",
       "      <th>Attractive</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Oval_Face</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000002_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000016_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>000026_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000069_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>000104_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162679</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162679_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162684</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162684_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162706</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162706_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162752</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162752_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162763</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162763_64.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5364 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Female  Young  Attractive  Smiling  Oval_Face  Wavy_Hair  \\\n",
       "2          1.0    0.0         1.0      1.0        0.0        0.0   \n",
       "16         0.0    0.0         1.0      1.0        1.0        0.0   \n",
       "26         1.0    0.0         1.0      1.0        1.0        1.0   \n",
       "69         0.0    0.0         1.0      1.0        0.0        0.0   \n",
       "104        1.0    0.0         1.0      1.0        0.0        1.0   \n",
       "...        ...    ...         ...      ...        ...        ...   \n",
       "162679     0.0    0.0         1.0      1.0        0.0        0.0   \n",
       "162684     0.0    0.0         1.0      1.0        0.0        0.0   \n",
       "162706     1.0    0.0         1.0      1.0        1.0        0.0   \n",
       "162752     1.0    0.0         1.0      1.0        0.0        0.0   \n",
       "162763     0.0    0.0         1.0      0.0        0.0        0.0   \n",
       "\n",
       "                 path  \n",
       "2       000002_64.jpg  \n",
       "16      000016_64.jpg  \n",
       "26      000026_64.jpg  \n",
       "69      000069_64.jpg  \n",
       "104     000104_64.jpg  \n",
       "...               ...  \n",
       "162679  162679_64.jpg  \n",
       "162684  162684_64.jpg  \n",
       "162706  162706_64.jpg  \n",
       "162752  162752_64.jpg  \n",
       "162763  162763_64.jpg  \n",
       "\n",
       "[5364 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query for estimate attr by gender\n",
    "labels.query(\"Young == 0.0 and Attractive == 1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female = 0 and Attractive = 1 : 19014\n",
    "# Female = 1 and Attractive = 1 : 64589\n",
    "\n",
    "# Young = 0 and Attractive = 1 : 5364\n",
    "# Young = 1 and Attractive = 1 : 78239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: (62437, 7)\n",
      "B2: (4255, 7)\n",
      "B3: (38926, 7)\n",
      "B4: (24598, 7)\n"
     ]
    }
   ],
   "source": [
    "B1 = labels_train[(labels_train[\"Young\"] == 1) & (labels_train[\"Attractive\"] == 1)]\n",
    "B2 = labels_train[(labels_train[\"Young\"] == 0) & (labels_train[\"Attractive\"] == 1)]\n",
    "B3 = labels_train[(labels_train[\"Young\"] == 1) & (labels_train[\"Attractive\"] == 0)]\n",
    "B4 = labels_train[(labels_train[\"Young\"] == 0) & (labels_train[\"Attractive\"] == 0)]\n",
    "\n",
    "print(\"B1:\", B1.shape)\n",
    "print(\"B2:\", B2.shape)\n",
    "print(\"B3:\", B3.shape)\n",
    "print(\"B4:\", B4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: (62437, 7)\n",
      "B2: (62437, 7)\n"
     ]
    }
   ],
   "source": [
    "diff = len(B1)-len(B2)\n",
    "oversampled_B2 = B2.sample(n=diff, replace=True, random_state=42)\n",
    "B2_surech = pd.concat([B2, oversampled_B2], ignore_index=True)\n",
    "\n",
    "print(\"B1:\", B1.shape)\n",
    "print(\"B2:\", B2_surech.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B3: (38926, 7)\n",
      "B4: (38926, 7)\n"
     ]
    }
   ],
   "source": [
    "diff = len(B3)-len(B4)\n",
    "oversampled_B4 = B4.sample(n=diff, replace=True, random_state=42)\n",
    "B4_surech = pd.concat([B4, oversampled_B4], ignore_index=True)\n",
    "\n",
    "print(\"B3:\", B3.shape)\n",
    "print(\"B4:\", B4_surech.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "044530_64.jpg    29\n",
       "030385_64.jpg    28\n",
       "006440_64.jpg    28\n",
       "033828_64.jpg    28\n",
       "072062_64.jpg    28\n",
       "                 ..\n",
       "103946_64.jpg     5\n",
       "161583_64.jpg     5\n",
       "088518_64.jpg     5\n",
       "161961_64.jpg     5\n",
       "002117_64.jpg     5\n",
       "Name: count, Length: 4255, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B2_surech.path.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202726, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [B1, B2_surech, B3, B4_surech]\n",
    "\n",
    "labels_di = pd.concat(dataframes, ignore_index=True)\n",
    "labels_di.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_di = labels_di.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Female/Attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female</th>\n",
       "      <th>Young</th>\n",
       "      <th>Attractive</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Oval_Face</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000013_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>000018_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000019_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000023_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>000028_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162753</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162753_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162756</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162756_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162757</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162757_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162758</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162758_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162761</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162761_64.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29920 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Female  Young  Attractive  Smiling  Oval_Face  Wavy_Hair  \\\n",
       "13         1.0    1.0         0.0      0.0        0.0        0.0   \n",
       "18         1.0    0.0         0.0      0.0        0.0        1.0   \n",
       "19         1.0    1.0         0.0      1.0        0.0        0.0   \n",
       "23         1.0    1.0         0.0      0.0        0.0        0.0   \n",
       "28         1.0    1.0         0.0      0.0        0.0        0.0   \n",
       "...        ...    ...         ...      ...        ...        ...   \n",
       "162753     1.0    1.0         0.0      0.0        0.0        0.0   \n",
       "162756     1.0    1.0         0.0      0.0        0.0        0.0   \n",
       "162757     1.0    0.0         0.0      0.0        0.0        1.0   \n",
       "162758     1.0    0.0         0.0      1.0        0.0        1.0   \n",
       "162761     1.0    1.0         0.0      0.0        0.0        0.0   \n",
       "\n",
       "                 path  \n",
       "13      000013_64.jpg  \n",
       "18      000018_64.jpg  \n",
       "19      000019_64.jpg  \n",
       "23      000023_64.jpg  \n",
       "28      000028_64.jpg  \n",
       "...               ...  \n",
       "162753  162753_64.jpg  \n",
       "162756  162756_64.jpg  \n",
       "162757  162757_64.jpg  \n",
       "162758  162758_64.jpg  \n",
       "162761  162761_64.jpg  \n",
       "\n",
       "[29920 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query for estimate attr by gender\n",
    "labels.query(\"Female == 1.0 and Attractive == 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female = 1 and Attractive = 1 : 64589\n",
    "# Female = 0 and Attractive = 1 : 19014\n",
    "# x3.397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female</th>\n",
       "      <th>Young</th>\n",
       "      <th>Attractive</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Oval_Face</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151255_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127003_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>067045_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>107007_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108829_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202670</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110775_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202684</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>149154_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202685</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>053129_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202699</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>026284_64.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202706</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126161_64.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27488 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Female  Young  Attractive  Smiling  Oval_Face  Wavy_Hair  \\\n",
       "0          1.0    0.0         0.0      1.0        0.0        0.0   \n",
       "3          1.0    0.0         0.0      1.0        0.0        1.0   \n",
       "7          1.0    1.0         0.0      0.0        1.0        0.0   \n",
       "11         1.0    0.0         0.0      1.0        1.0        1.0   \n",
       "21         1.0    1.0         0.0      0.0        0.0        0.0   \n",
       "...        ...    ...         ...      ...        ...        ...   \n",
       "202670     1.0    0.0         0.0      0.0        0.0        1.0   \n",
       "202684     1.0    0.0         0.0      0.0        1.0        1.0   \n",
       "202685     1.0    1.0         0.0      0.0        0.0        0.0   \n",
       "202699     1.0    1.0         0.0      1.0        0.0        1.0   \n",
       "202706     1.0    0.0         0.0      1.0        0.0        1.0   \n",
       "\n",
       "                 path  \n",
       "0       151255_64.jpg  \n",
       "3       127003_64.jpg  \n",
       "7       067045_64.jpg  \n",
       "11      107007_64.jpg  \n",
       "21      108829_64.jpg  \n",
       "...               ...  \n",
       "202670  110775_64.jpg  \n",
       "202684  149154_64.jpg  \n",
       "202685  053129_64.jpg  \n",
       "202699  026284_64.jpg  \n",
       "202706  126161_64.jpg  \n",
       "\n",
       "[27488 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query for estimate attr by gender\n",
    "labels_di.query(\"Female == 1.0 and Attractive == 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female = 1 and Attractive = 1 : 126246\n",
    "# Female = 0 and Attractive = 1 : 48163\n",
    "# x2.621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: (91091, 7)\n",
      "B2: (33783, 7)\n",
      "B3: (27488, 7)\n",
      "B4: (50364, 7)\n"
     ]
    }
   ],
   "source": [
    "B1 = labels_di[(labels_di[\"Female\"] == 1) & (labels_di[\"Attractive\"] == 1)]\n",
    "B2 = labels_di[(labels_di[\"Female\"] == 0) & (labels_di[\"Attractive\"] == 1)]\n",
    "B3 = labels_di[(labels_di[\"Female\"] == 1) & (labels_di[\"Attractive\"] == 0)]\n",
    "B4 = labels_di[(labels_di[\"Female\"] == 0) & (labels_di[\"Attractive\"] == 0)]\n",
    "\n",
    "print(\"B1:\", B1.shape)\n",
    "print(\"B2:\", B2.shape)\n",
    "print(\"B3:\", B3.shape)\n",
    "print(\"B4:\", B4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: (91091, 7)\n",
      "B2: (91091, 7)\n"
     ]
    }
   ],
   "source": [
    "diff = len(B1)-len(B2)\n",
    "oversampled_B2 = B2.sample(n=diff, replace=True, random_state=42)\n",
    "B2_surech = pd.concat([B2, oversampled_B2], ignore_index=True)\n",
    "\n",
    "print(\"B1:\", B1.shape)\n",
    "print(\"B2:\", B2_surech.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B3: (50364, 7)\n",
      "B4: (50364, 7)\n"
     ]
    }
   ],
   "source": [
    "diff = len(B4)-len(B3)\n",
    "oversampled_B3 = B3.sample(n=diff, replace=True, random_state=42)\n",
    "B3_surech = pd.concat([B3, oversampled_B3], ignore_index=True)\n",
    "\n",
    "print(\"B3:\", B3_surech.shape)\n",
    "print(\"B4:\", B4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282910, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [B1, B2_surech, B3_surech, B4]\n",
    "\n",
    "labels_di = pd.concat(dataframes, ignore_index=True)\n",
    "labels_di.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_di = labels_di.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, labels, root):\n",
    "        self.img_labels = labels\n",
    "        self.root = root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.root + self.img_labels['path'].iloc[index]\n",
    "        image = open_image(img_path, numpy_format=True)\n",
    "        label = self.img_labels.iloc[index, :6].to_numpy().astype('float')\n",
    "        # Convert image and label to torch tensors\n",
    "        image = torch.from_numpy(image)\n",
    "        label = torch.from_numpy(label).float()\n",
    "        # Pytorch works with C x H x W format\n",
    "        image = image.permute(2, 0, 1)\n",
    "        # Normalize the image\n",
    "        image = image/255\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "dataset = CustomDataset(labels, root)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, test_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloader for train and test datasets\n",
    "batch_size = 512\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader for DI correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "dataset_di = CustomDataset(labels_di, root)\n",
    "dataset_test = CustomDataset(labels_test, root)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "# generator = torch.Generator().manual_seed(42)\n",
    "# train_size_di = int(0.8 * len(dataset_di))\n",
    "# test_size_di = len(dataset_di) - train_size_di\n",
    "# train_dataset_di, test_dataset_di = torch.utils.data.random_split(\n",
    "#     dataset_di, [train_size_di, test_size_di], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the dataloader for train and test datasets\n",
    "# batch_size = 512\n",
    "# train_dataloader_di = DataLoader(\n",
    "#     train_dataset_di, batch_size=batch_size, shuffle=True)\n",
    "# test_dataloader_di = DataLoader(\n",
    "#     test_dataset_di, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloader for train and test datasets\n",
    "batch_size = 512\n",
    "train_dataloader_di = DataLoader(\n",
    "    dataset_di, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_di = DataLoader(\n",
    "    dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset_di' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/glab/Desktop/kaggle/code/processing.ipynb Cell 41\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Display the first row of train dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_dataset_di\u001b[39m.\u001b[39mdataset[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset_di' is not defined"
     ]
    }
   ],
   "source": [
    "# Display the first row of train dataset\n",
    "train_dataset_di.dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric and score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    y_pred = (y_pred > 0.5).float()\n",
    "    return (y_pred == y_true).float().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disparate_impact_fem(prediction, labels):\n",
    "    numerator = (prediction[:, 2] == 1) & (labels[:, 0] == 1)\n",
    "    denominator = (prediction[:, 2] == 1) & (labels[:, 0] == 0)\n",
    "    return numerator.sum() / (denominator.sum() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disparate_impact_youn(prediction, labels):\n",
    "    numerator = (prediction[:, 2] == 1) & (labels[:, 1] == 1)\n",
    "    denominator = (prediction[:, 2] == 1) & (labels[:, 1] == 0)\n",
    "    return numerator.sum() / (denominator.sum() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_score(prediction, labels):\n",
    "\n",
    "    accuracy_attr = accuracy(prediction[:, 2], labels[:, 2])\n",
    "\n",
    "    DI_attr_fem = disparate_impact_fem(prediction, labels)\n",
    "    DI_attr_youn = disparate_impact_youn(prediction, labels)\n",
    "\n",
    "    accuracy_fem = accuracy(prediction[:, 0], labels[:, 0])\n",
    "    accuracy_youn = accuracy(prediction[:, 1], labels[:, 1])\n",
    "    accuracy_smil = accuracy(prediction[:, 3], labels[:, 3])\n",
    "    accuracy_oval = accuracy(prediction[:, 4], labels[:, 4])\n",
    "    accuracy_wavy = accuracy(prediction[:, 5], labels[:, 5])\n",
    "    accuracy_label = (accuracy_fem + accuracy_youn + accuracy_smil + accuracy_oval + accuracy_wavy) / 5\n",
    "\n",
    "    return 0.3 * (accuracy_attr-5*(DI_attr_youn-1)-2*(DI_attr_fem-1)) + 0.5 * accuracy_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.conv_bn_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(18432, 64), nn.ReLU(), nn.Linear(64, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_bn_relu_stack(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.conv_bn_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(18432, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_bn_relu_stack(x)\n",
    "        x = self.flatten(x)\n",
    "        z = self.linear(x)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.end = nn.Sequential(\n",
    "            nn.ReLU(), nn.Linear(64, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        y = self.end(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(64, 128*8*8),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, 8, 8)),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, stride=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        y = self.decoder(z)\n",
    "        return y\n",
    "    \n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "autoencoder = AutoEncoder(encoder, decoder).to(device)\n",
    "cnn = CNN2(encoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "autoencoder = AutoEncoder(encoder, decoder).to(device)\n",
    "cnn = CNN2(encoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "sigmoid = nn.Sigmoid()\n",
    "bce = nn.BCELoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1986478563.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[84], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    ims = ims.to(device)a\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 5\n",
    "# Initialization\n",
    "train_losses_dict = {}\n",
    "test_losses_dict = {}\n",
    "scores = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # List to store losses\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for phase in ['train', 'test']:\n",
    "        dl = train_dataloader if phase == 'train' else test_dataloader\n",
    "        # Bar to display progress\n",
    "        for batch in (bar := tqdm(iter(dl))):\n",
    "            ims, gt = batch\n",
    "            # Move the images and gt to the device (like the model)\n",
    "            ims = ims.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            output = sigmoid(model(ims))\n",
    "            loss = bce(output, gt)\n",
    "            # Minimize custom score\n",
    "            # loss = custom_score(output, gt)\n",
    "            prediction = (output > 0.5).float()\n",
    "            score = custom_score(prediction, gt)\n",
    "            scores.append(score.item())\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_losses.append(loss.item())\n",
    "                # Backpropagation and gradient descent\n",
    "                # loss.requires_grad = True\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                bar.set_description(\n",
    "                    f\"Train {epoch+1} loss: {np.mean(train_losses).item():.4f}, Train score: {score.item():.4f}\")\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                test_losses.append(loss.item())\n",
    "                bar.set_description(\n",
    "                    f\"Test {epoch+1} loss: {np.mean(test_losses).item():.4f}, Test score: {score.item():.4f}\")\n",
    "        \n",
    "        if phase == 'train':\n",
    "            train_losses_dict[epoch+1] = np.mean(train_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [dataset_di[i][1] for i in range(9000, 9000+9000)]\n",
    "a = torch.stack(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 0., 1.],\n",
       "        [1., 0., 1., 1., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 1., 1., 0., 0.],\n",
       "        [1., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2725)\n",
      "tensor(3443)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7915)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disparate_impact_youn(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., 1., 0., 1.],\n",
       "        [1., 0., 1., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1 loss: 0.3656, Train score: 69.2896, Train DI youn: 1.0714, Train DI fem: 1.1324, lr : 0.001000: 100%|██████████| 555/555 [02:01<00:00,  4.55it/s]\n",
      "Test 1 loss: 0.3578, Test score: 48.6085, Test DI youn: 12.1429, Test DI fem: 2.7551: 100%|██████████| 64/64 [00:11<00:00,  5.49it/s]\n",
      "Train 2 loss: 0.2664, Train score: 70.0564, Train DI youn: 0.8158, Train DI fem: 0.8904, lr : 0.000900: 100%|██████████| 555/555 [01:59<00:00,  4.63it/s]\n",
      "Test 2 loss: 0.3717, Test score: 48.4654, Test DI youn: 11.7692, Test DI fem: 3.3684: 100%|██████████| 64/64 [00:11<00:00,  5.48it/s]\n",
      "Train 3 loss: 0.2288, Train score: 72.1309, Train DI youn: 0.7802, Train DI fem: 1.0903, lr : 0.000810:  36%|███▌      | 200/555 [00:42<01:15,  4.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/glab/Desktop/kaggle/code/processing.ipynb Cell 63\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y115sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m dl \u001b[39m=\u001b[39m train_dataloader_di \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m test_dataloader_di\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y115sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Bar to display progress\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y115sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m (bar \u001b[39m:=\u001b[39m tqdm(\u001b[39miter\u001b[39m(dl))):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y115sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     ims, gt \u001b[39m=\u001b[39m batch\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y115sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# Move the images and gt to the device (like the model)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/glab/Desktop/kaggle/code/processing.ipynb Cell 63\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y115sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_labels[\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y115sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m image \u001b[39m=\u001b[39m open_image(img_path, numpy_format\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y115sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimg_labels\u001b[39m.\u001b[39;49miloc[index, :\u001b[39m6\u001b[39;49m]\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y115sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Convert image and label to torch tensors\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y115sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(image)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1097\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1096\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m-> 1097\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m   1098\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1596\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1594\u001b[0m tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_tuple_indexer(tup)\n\u001b[1;32m   1595\u001b[0m \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m-> 1596\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_lowerdim(tup)\n\u001b[1;32m   1598\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1024\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[39mreturn\u001b[39;00m section\n\u001b[1;32m   1023\u001b[0m         \u001b[39m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[0;32m-> 1024\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(section, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)[new_key]\n\u001b[1;32m   1026\u001b[0m \u001b[39mraise\u001b[39;00m IndexingError(\u001b[39m\"\u001b[39m\u001b[39mnot applicable\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1633\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1627\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[1;32m   1628\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataFrame indexer is not allowed for .iloc\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1629\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using .loc for automatic alignment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1630\u001b[0m     )\n\u001b[1;32m   1632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n\u001b[0;32m-> 1633\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_slice_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1635\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   1636\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:1669\u001b[0m, in \u001b[0;36m_iLocIndexer._get_slice_axis\u001b[0;34m(self, slice_obj, axis)\u001b[0m\n\u001b[1;32m   1667\u001b[0m labels \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1668\u001b[0m labels\u001b[39m.\u001b[39m_validate_positional_slice(slice_obj)\n\u001b[0;32m-> 1669\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_slice(slice_obj, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:990\u001b[0m, in \u001b[0;36mSeries._slice\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slice\u001b[39m(\u001b[39mself\u001b[39m, slobj: \u001b[39mslice\u001b[39m \u001b[39m|\u001b[39m np\u001b[39m.\u001b[39mndarray, axis: Axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[1;32m    988\u001b[0m     \u001b[39m# axis kwarg is retained for compat with NDFrame method\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[39m#  _slice is *always* positional\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_values(slobj)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 5\n",
    "# Initialization\n",
    "train_losses_dict = {}\n",
    "test_losses_dict = {}\n",
    "scores = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # List to store losses\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for phase in ['train', 'test']:\n",
    "        dl = train_dataloader_di if phase == 'train' else test_dataloader_di\n",
    "        # Bar to display progress\n",
    "        for batch in (bar := tqdm(iter(dl))):\n",
    "            ims, gt = batch\n",
    "            # Move the images and gt to the device (like the model)\n",
    "            ims = ims.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            output = sigmoid(model(ims))\n",
    "            loss = bce(output, gt)\n",
    "            # Minimize custom score\n",
    "            # loss = custom_score(output, gt)\n",
    "            prediction = (output > 0.5).float()\n",
    "            score = custom_score(prediction, gt)\n",
    "            di_youn = disparate_impact_youn(prediction, gt)\n",
    "            di_fem = disparate_impact_fem(prediction, gt)\n",
    "            scores.append(score.item())\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_losses.append(loss.item())\n",
    "                # Backpropagation and gradient descent\n",
    "                # loss.requires_grad = True\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                bar.set_description(\n",
    "                    f\"Train {epoch+1} loss: {np.mean(train_losses).item():.4f}, Train score: {score.item():.4f}, Train DI youn: {di_youn.item():.4f}, Train DI fem: {di_fem.item():.4f}, lr : {scheduler.get_last_lr()[0]:.6f}\")\n",
    "            else:\n",
    "                test_losses.append(loss.item())\n",
    "                bar.set_description(\n",
    "                    f\"Test {epoch+1} loss: {np.mean(test_losses).item():.4f}, Test score: {score.item():.4f}, Test DI youn: {di_youn.item():.4f}, Test DI fem: {di_fem.item():.4f}\")\n",
    "\n",
    "        if phase == 'train':\n",
    "            train_losses_dict[epoch+1] = np.mean(train_losses).item()\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018110275268554688\n",
      "0.04068635031580925\n",
      "0.02394815906882286\n",
      "0.022823866456747055\n",
      "0.024724673479795456\n",
      "0.023918485268950462\n",
      "0.023232821375131607\n",
      "0.022722532972693443\n",
      "0.02146877720952034\n",
      "0.02073671668767929\n",
      "0.020907919853925705\n",
      "0.020678043365478516\n",
      "0.020357241854071617\n",
      "0.02063966915011406\n",
      "0.01984921097755432\n",
      "0.01948319375514984\n",
      "0.01896766573190689\n",
      "0.018942466005682945\n",
      "0.01851677894592285\n",
      "0.018228501081466675\n",
      "0.017713239416480064\n",
      "0.017917297780513763\n",
      "0.01811206340789795\n",
      "0.017723308876156807\n",
      "0.017804473638534546\n",
      "0.017009668052196503\n",
      "0.017142999917268753\n",
      "0.01738348975777626\n",
      "0.017712406814098358\n",
      "0.015977708622813225\n",
      "0.017066918313503265\n",
      "0.016987930983304977\n",
      "0.016675248742103577\n",
      "0.016637658700346947\n",
      "0.017048362642526627\n",
      "0.01642046496272087\n",
      "0.016139525920152664\n",
      "0.016576290130615234\n",
      "0.016148898750543594\n",
      "0.015993576496839523\n",
      "0.016750723123550415\n",
      "0.01591888628900051\n",
      "0.015595363453030586\n",
      "0.01627380959689617\n",
      "0.01605970226228237\n",
      "0.015807384625077248\n",
      "0.01575586386024952\n",
      "0.015973880887031555\n",
      "0.01564072072505951\n",
      "0.015449309721589088\n",
      "0.015754051506519318\n",
      "0.015259632840752602\n",
      "0.01553756557404995\n",
      "0.015265688300132751\n",
      "0.01574699580669403\n",
      "0.014965726062655449\n",
      "0.01554972492158413\n",
      "0.014840411953628063\n",
      "0.014782704412937164\n",
      "0.014602857641875744\n",
      "0.014971784316003323\n",
      "0.014670141041278839\n",
      "0.01423385739326477\n",
      "0.014297030866146088\n",
      "0.014488901011645794\n",
      "0.014869610778987408\n",
      "0.014784909784793854\n",
      "0.014305917546153069\n",
      "0.014351306483149529\n",
      "0.01430570986121893\n",
      "0.01477418839931488\n",
      "0.014084575697779655\n",
      "0.014698173850774765\n",
      "0.014097507111728191\n",
      "0.014160829596221447\n",
      "0.014425295405089855\n",
      "0.014106865040957928\n",
      "0.013827718794345856\n",
      "0.014359827153384686\n",
      "0.013947617262601852\n",
      "0.013793673366308212\n",
      "0.013606006279587746\n",
      "0.013553962111473083\n",
      "0.013906151056289673\n",
      "0.013551941141486168\n",
      "0.013476850464940071\n",
      "0.013982991687953472\n",
      "0.01335604302585125\n",
      "0.01376273762434721\n",
      "0.013867574743926525\n",
      "0.014289824292063713\n",
      "0.013286326080560684\n",
      "0.013497854582965374\n",
      "0.013162162154912949\n",
      "0.012893633916974068\n",
      "0.013182267546653748\n",
      "0.013649857603013515\n",
      "0.013359583914279938\n",
      "0.013517327606678009\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/glab/Desktop/kaggle/code/processing.ipynb Cell 66\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y122sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m output \u001b[39m=\u001b[39m decoder(encoder(img))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y122sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, img)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y122sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y122sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y122sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Train the autoencoder\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        img, _ = data\n",
    "        img = img.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = decoder(encoder(img))\n",
    "        loss = criterion(output, img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Print the loss\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, train_loss / len(train_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty nvidia\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with correct name\n",
    "torch.save(model.state_dict(), '../models/cnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores rolling mean\n",
    "scores_roll = pd.Series(scores)\n",
    "scores_roll = scores_roll.rolling(10).mean()\n",
    "plt.plot(scores_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export for submission¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class KaggleDataset(Dataset):\n",
    "    def __init__(self, root_kaggle):\n",
    "        self.root_kaggle = root_kaggle\n",
    "        self.files_list = sorted(glob(self.root_kaggle + \"*.jpg\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.files_list[index]\n",
    "        image = open_image(img_path, numpy_format=True)\n",
    "        # Convert image and label to torch tensors\n",
    "        image = torch.from_numpy(image)\n",
    "        # Pytorch works with C x H x W format\n",
    "        image = image.permute(2, 0, 1)\n",
    "        # Normalize the image\n",
    "        image = image/255\n",
    "\n",
    "        return image, img_path.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "kaggle_dataset = KaggleDataset(root_kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloader for train and test datasets\n",
    "batch_size = 256\n",
    "kaggle_dataloader = DataLoader(\n",
    "    kaggle_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.91it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_list = []\n",
    "paths_list = []\n",
    "for batch in (bar:= tqdm(iter(kaggle_dataloader))):\n",
    "    images, paths = batch\n",
    "    # Move the images and gt to the device (like the model)\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outputs = sigmoid(model(images))\n",
    "    # Append the prediction to the a list\n",
    "    predictions = list((outputs > 0.5).detach().cpu().numpy().astype(int))\n",
    "    predictions_list.extend(predictions)\n",
    "    paths_list.extend(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ID  Female  Young  Attractive  Smiling  Oval_Face  Wavy_Hair\n",
      "0  000000_64.jpg       0      1           1        0          0          0\n",
      "1  000001_64.jpg       0      1           1        0          1          1\n",
      "2  000002_64.jpg       1      1           1        1          1          1\n",
      "3  000003_64.jpg       1      1           1        0          1          0\n",
      "4  000004_64.jpg       0      0           0        1          1          0\n"
     ]
    }
   ],
   "source": [
    "col_names = ['ID', 'Female', 'Young', 'Attractive', 'Smiling', 'Oval_Face', 'Wavy_Hair']\n",
    "df = pd.DataFrame({'ID': paths_list})\n",
    "\n",
    "for i, col_name in enumerate(col_names[1:]):\n",
    "    df[col_name] = [p[i] for p in predictions_list]\n",
    "\n",
    "print(df.head())\n",
    "# +1 for version\n",
    "df.to_csv('../subs/submission01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/glab/Desktop/kaggle/code/processing.ipynb Cell 78\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y133sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 19962 is the correct value for submit\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/glab/Desktop/kaggle/code/processing.ipynb#Y133sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mlen\u001b[39m(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# 19962 is the correct value for submit\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
